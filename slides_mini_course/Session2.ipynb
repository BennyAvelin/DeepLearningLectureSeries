{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to deep learning\n",
    "Benny Avelin\n",
    "<p><a href=\"https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg#/media/File:Colored_neural_network.svg\">\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png\" width=500px alt=\"Colored neural network.svg\">\n",
    "        </a>\n",
    "        </center>\n",
    "        <br>\n",
    "        <font size=\"1\">By <a href=\"//commons.wikimedia.org/wiki/User_talk:Glosser.ca\" title=\"User talk:Glosser.ca\">Glosser.ca</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, Derivative of <a href=\"//commons.wikimedia.org/wiki/File:Artificial_neural_network.svg\" title=\"File:Artificial neural network.svg\">File:Artificial neural network.svg</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=24913461\">Link</a>\n",
    "    </p>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Definitions (Skip)\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\H}{\\mathcal{H}}$\n",
    "$\\newcommand{\\VCdim}{\\text{VC-dim}(\\H)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview (session 2)\n",
    "\n",
    "* Quick recap of the definition\n",
    "* Detour into VC-theory and generalization bounds\n",
    "* Why Deep Neural Networks are difficult\n",
    "* Open problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Previously\n",
    "A single artificial neuron can then be represented as\n",
    "$$\n",
    "    h(x) = \\sigma(w \\cdot x + b)\n",
    "$$\n",
    "where $w$Â and $b$ are the weights. $\\sigma:\\R \\to \\R$ is an activation function, for instance\n",
    "* Sigmoid $\\frac{1}{1+e^{-x}}$\n",
    "* ReLu (Rectified Linear unit) $\\sigma(x)=\\max(0,x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Previously\n",
    "We can make more complicated models by adding more neurons, i.e. let us consider:\n",
    "$$\n",
    "    f(x) = \\sum_{j=1}^k v^j h_{w^j,b^j}(x)\n",
    "$$\n",
    "\n",
    "this is what is known as a feed forward neural network with a single hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"SingleHidden.png\" width=450px>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the context of neural networks we let $\\sigma: \\mathbb{R} \\to \\mathbb{R}$ and with $\\sigma(x)$ where $x \\in \\mathbb{R}^n$ we apply componentwise. Denote\n",
    "\n",
    "$$W^T = (w^1,\\ldots, w^k)$$\n",
    "$$B^T = (b_1,\\ldots,b_k)$$\n",
    "$$v^T = (v_1,\\ldots,v_k)$$\n",
    "\n",
    "then we can write $f$ as\n",
    "$$\n",
    "    f(x) = v^T \\sigma(Wx+B)\n",
    "$$\n",
    "\n",
    "In fact if we introduce $W^{(1)},B^{(1)},W^{(2)},B^{(2)}$ we can consider\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f^{(1)}(x) &= \\sigma(W^{(1)} x + B^{(1)})\\\\\n",
    "    f^{(2)}(x) &= \\sigma(W^{(2)} f^{(1)}(x) + B^{(2)}) \\\\\n",
    "    f(x) &= v^T f^{(2)}(x)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"ManyHidden.png\" width=700px>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Risk & hypothesis\n",
    "* Let us consider data $(x,y) \\sim \\mu$, where $x \\in \\R^n$ and $y \\in \\R^m$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A hypothesis is a function $h: \\R^n \\to \\R^m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A loss-function $L:\\R^m \\times \\R^m \\to \\R_+$\n",
    "$$R(h) = \\E_{\\mu}\\left[L(h(x),y)\\right], \\quad \\textbf{Risk}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Given a data-set $D = \\{(x_1,y_1), \\ldots (x_N,y_N)\\}$ which are sampled i.i.d. from $\\mu$ we also define\n",
    "$$R_{emp,D} (h) = \\frac{1}{N}\\sum_{i=1}^N \\left[L(h(x_i),y_i)\\right], \\quad \\textbf{Empirical Risk}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Call a set of hypothesis $\\H$ the hypothesis space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goal\n",
    "\n",
    "* Find $h^\\ast \\in \\H$ such that\n",
    "$$R(h^\\ast) = \\min_{\\H} R(h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We dont have access to $\\mu$ but we have access to the data-set $D$, we could try to find $h_D^\\ast \\in \\H$ such that\n",
    "$$R_{emp,D}(h_D^\\ast) = \\min_{\\H} R_{emp,D}(h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We cannot find $h_D^\\ast$ in general. Instead we try to find $h \\in \\H$ such that $R_{emp,D}(h)$ is as small as possible\n",
    "$$R_{emp,D}(h_D^\\ast) \\leq R_{emp,D}(h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalization\n",
    "The **generalization gap** is the difference between $R(h)$ and $R_{emp,D}(h),$ we would like to know that\n",
    "\n",
    "$$\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}} |R(h) - R_{\\text{emp}}(h)| > \\epsilon\\right] \\quad \\text{is small}$$\n",
    "\n",
    "We are taking the probability over data-sets $D$ sampled from $\\mu$ all with the same size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Given $h$ we get from the weak law of large numbers that\n",
    "$$\\lim_{N \\rightarrow \\infty} \\mathbb{P}\\left[\\left|R(h) - R_{\\text{emp}}(h)\\right| > \\epsilon \\right] = 0, \\quad \\text{too weak}$$\n",
    "$N$ is the sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Hoeffdings inequality (Assuming $R(h), R_{\\text{emp}}(h) \\leq 1$):\n",
    "$$\\mathbb{P}\\left[|R(h) - R_{\\text{emp}}(h)| > \\epsilon \\right ] \\leq 2\\exp(-2N\\epsilon^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* If $|\\H|$ is finite we could use the union bound and Hoeffdings inequality to get\n",
    "$$\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}|R(h) - R_\\text{emp}(h)| > \\epsilon\\right] \\leq \n",
    "2|\\mathcal{H}|\\exp(-2N\\epsilon^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Symmetrization Lemma (Vapnik, Chervonenkis):** Let $D$ and $D'$ be two different sample data-sets from $\\mu$ then\n",
    "$$\\begin{multline}\n",
    "    \\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}\\left|R(h) - R_\\text{emp,D}(h)\\right| > \\epsilon\\right] \\\\\n",
    "    \\leq 2\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}\\left|R_\\text{emp,D}(h) - R_\\text{emp,D'}(h)\\right| > \\frac{\\epsilon}{2}\\right]\n",
    "\\end{multline}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Lets simplify things. Assume that our hypothesis has output in $\\{-1,1\\}$ i.e. two classes.\n",
    "* Define data-dependent equivalence classes on $\\H$ with respect to $D$\n",
    "$$h_1 = h_2, \\iff h_1(x) = h_2(x), \\forall (x,.) \\in D$$\n",
    "* The set of classes is denoted $\\H_D$, note that $|\\H_D| < \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{multline}\n",
    "    \\mathbb{P}\\left[\\sup_{h \\in \\H_{D\\cup D'}}\\left|R_\\text{emp}(h) - R_\\text{emp}'(h)\\right|>\\frac{\\epsilon}{2}\\right] \\\\\n",
    "    \\leq s(\\H,N) \\sup_{h \\in \\H} \\mathbb{P}\\left[\\left|R_\\text{emp}(h) - R_\\text{emp}'(h)\\right|>\\frac{\\epsilon}{2}\\right]\n",
    "\\end{multline}\n",
    "$$\n",
    "where $s$ is an upper bound on $\\|\\H_{D \\cup D'}\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Growth function (shattering number)\n",
    "\n",
    "The largest size of $\\H_D$ for a given $N$ is called the shattering number for $\\H$ given $N$\n",
    "$$s(\\H,N) = \\sup_{x_1,\\ldots, x_N} |\\H_{\\{x_1,\\ldots,x_N\\}}|$$\n",
    "> **Note:** this is a combinatorial number and does not depend on $\\mu$. Some call this distribution free learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, for any two data-sets $D,D'$ we get\n",
    "$$\n",
    "\\begin{multline}\n",
    "    \\mathbb{P}\\left[\\sup_{h \\in \\H_{D\\cup D'}}\\left|R_\\text{emp}(h) - R_\\text{emp}'(h)\\right|>\\frac{\\epsilon}{2}\\right] \\\\\n",
    "    \\leq s(\\H,N) \\sup_{h \\in \\H}  \\mathbb{P}\\left[\\left|R_\\text{emp}(h) - R_\\text{emp}'(h)\\right|>\\frac{\\epsilon}{2}\\right]\n",
    "\\end{multline}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can we bound $s(\\H,N)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If we knew nothing then the best we can do is (binary classification)\n",
    "$$s(\\H,N) \\leq 2^N$$\n",
    "Remember: Hoeffdings inequality gave a decay of $\\exp(-2N\\epsilon^2)$. **Too weak**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This is where **VC**(Vapnik Chervonenkis)-dimension comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vapnik Chervonenkis dimension\n",
    "**Def:** We say that $\\H$ **shatters** a set $D = \\{x_i,i=1,\\ldots, N\\}$ for any disjoint split $D_1,D_{-1}$ of $D$ we can find $h \\in \\H$ such that $h(D_1) = 1$, $h(D_{-1})=-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Def:** The VC-dimension of $\\H$, denoted by $\\text{VC-dim}(\\H)$ , equals the largest integer $n$ such that there exists a set of cardinality $n$ that is shattered by $\\H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SauerâShelah lemma (1972)\n",
    "Let $k = \\text{VC-dim}(\\H)$ then for $N > 0$ we have\n",
    "$$\n",
    "    s(\\H,N) \\leq \\sum_{i=0}^{k-1} {N \\choose i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Independently proved by Sauer and Shelah in 1972\n",
    "* Actually proved slightly before by Vapnik and Chervonenkis (1971?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is polynomial:\n",
    "$$s(\\H,N) \\leq \\left ( \\frac{Ne}{k}\\right )^k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Proof: Let $\\lambda \\in (0,1)$ then\n",
    "$$\n",
    "\\tiny\n",
    "\\begin{align}\n",
    "    1 &= (\\lambda + (1-\\lambda))^N \\\\\n",
    "    &\\geq \\sum_{i=1}^{\\lambda N} {N \\choose i} \\lambda^i(1-\\lambda)^{n-1} \\\\\n",
    "    &\\geq \\sum_{i=1}^{\\lambda N} {N \\choose i} \\left (\\frac{\\lambda}{1-\\lambda} \\right )^{\\lambda n} (1-\\lambda)^n\n",
    "\\end{align}\n",
    "$$\n",
    "Thus\n",
    "$$\n",
    "\\tiny\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^{\\lambda N} {N \\choose i} &\\leq e^{N((\\lambda-1)\\log(1-\\lambda)-\\lambda \\log(1-\\lambda))} \\\\\n",
    "    &\\leq e^{N(\\lambda-\\lambda \\log(1-\\lambda))} \\\\\n",
    "    &= \\left ( \\frac{e N}{\\lambda N} \\right )^{\\lambda N}\n",
    "\\end{align}\n",
    "$$\n",
    "Then for $k = \\lambda N$ we have our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "$$\n",
    "\\tiny\n",
    "\\begin{align}\n",
    "    \\mathbb{P}&\\left[\\sup_{h \\in \\mathcal{H}}\\left|R(h) - R_\\text{emp,D}(h)\\right| > \\epsilon\\right] \\\\\n",
    "    &\\leq 2\\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}\\left|R_\\text{emp,D}(h) - R_\\text{emp,D'}(h)\\right| > \\frac{\\epsilon}{2}\\right] \\quad \\text{(Symmetrization Lemma)}\\\\\n",
    "    &\\leq 2 s(\\H,2 N) \\sup_{h \\in \\H}  \\mathbb{P}\\left[\\left|R_\\text{emp,D}(h) - R_\\text{emp,D'}(h)\\right|>\\frac{\\epsilon}{2}\\right] \\text{(Shattering number + Classes)}\\\\\n",
    "    &\\leq 2 s(\\H,2 N) \\sup_{h \\in \\H}   \\left ( \\mathbb{P}\\left[\\left|R(h) - R_\\text{emp,D}(h)\\right|>\\frac{\\epsilon}{4}\\right] + \\mathbb{P}\\left[\\left|R(h) - R_\\text{emp,D'}(h)\\right|>\\frac{\\epsilon}{4}\\right] \\right ) \\\\\n",
    "    &\\leq 8 s(\\H, 2N)\\exp \\left (-\\frac{N\\epsilon^2}{8} \\right ) \\quad \\text{(Hoeffdings Inequality)} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> The assumption $R(h) \\leq 1$ can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VC inequality (1971)\n",
    "\n",
    "$$\n",
    "\\begin{multline}\n",
    "    \\mathbb{P}\\left[\\sup_{h \\in \\mathcal{H}}\\left|R(h) - R_\\text{emp,D}(h)\\right| > \\epsilon\\right] \\\\\n",
    "    \\leq 8 s(\\H,2N) \\exp \\left (-\\frac{N\\epsilon^2}{8} \\right )\n",
    "\\end{multline}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# VC generalization bound\n",
    "Thus we can for any $\\delta$ find that with probability $(1-\\delta)$ the following estimate holds\n",
    "\n",
    "$$\n",
    "    R(h) \\leq R_{emp,D} + \\sqrt{\\frac{8 \\ln(s(\\H,2N))+8\\ln\\frac{8}{\\delta}}{N}}\n",
    "$$\n",
    "\n",
    "for any $h \\in \\H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VC generalization bound\n",
    "\n",
    "$$\n",
    "    R(h) \\leq R_{emp,D} + \\sqrt{\\frac{8 k\\ln\\left ( \\frac{2Ne}{k}\\right )+8\\ln\\frac{8}{\\delta}}{N}}\n",
    "$$\n",
    "We need $k \\lesssim N$ in order for the estimate to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "def find_N(k=1,prec=0.1):\n",
    "    delta=0.1\n",
    "    c = 8*k\n",
    "    t = 8*k*np.log(2*np.e/k)+8*np.log(8/delta)\n",
    "    func = lambda x: t+c*np.log(x)-prec*x\n",
    "    x_init = k*1000\n",
    "    return fsolve(func,x_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7835b083aa10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfind_N\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_numreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "k = np.arange(1,100)\n",
    "N = np.array([find_N(k0,1) for k0 in k]).ravel()\n",
    "def plot_numreq():\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(k,N)\n",
    "    plt.xlabel(\"VC-dimension = $k$\")\n",
    "    plt.ylabel(\"Datapoints required = $N$\")\n",
    "    plt.title(\"$\\\\frac{8 k\\\\ln\\\\left ( \\\\frac{2Ne}{k}\\\\right )+8\\\\ln\\\\frac{8}{\\\\delta}}{N} = 1, \\delta = 0.1$\",fontsize=15,pad=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_numreq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VC dimension of Neural networks\n",
    "Let $W$ be the total number of parameters of a network\n",
    "* (Cover 1968, Baum & Haussler 1989) LTU (Linear Threshold unit) \n",
    "$$\\VCdim \\leq 2 W \\log(eW)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* (Sontag 1992) $\\phi(x) = \\frac{1}{\\pi} \\arctan(x)+ \\frac{1}{2} + \\frac{cos(x)}{\\alpha (1+x^2)}$\n",
    "$$\\VCdim = \\infty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-10,10,1000)\n",
    "def plot_act(alpha=10):\n",
    "    y = (1/np.pi)*np.arctan(x)+0.5 + np.cos(x)/(alpha*(1+x**2))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(x,y)\n",
    "    plt.title(\"$\\\\frac{1}{\\\\pi} \\\\arctan(x)+ \\\\frac{1}{2} + \\\\frac{cos(x)}{alpha (1+x^2)}$\".replace('alpha',str(alpha)),fontsize=15,pad=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_act(alpha=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Proof\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\eta(x) &= \\chi_{\\{x \\geq 0\\}} \\\\\n",
    "    y &= \\eta(\\rho(x)), \\quad x \\in \\R \\\\\n",
    "    \\rho(x) &= \\phi(wx) + \\phi(-wx)-1 = \\frac{2 \\cos(wx)}{\\alpha(1+w^2x^2)}, \\quad w \\in \\R\n",
    "\\end{align}\n",
    "$$\n",
    "<center>\n",
    "    <img src=\"InfiniteVC.png\" width=600px>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Choose $x_1,\\ldots, x_n \\in \\R$ that are rationally independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. The set of $n$-tuples $(wx_1,\\ldots, wx_n)$ modulo $2\\pi$ is dense in $[0,2\\pi]^n$ for $w \\in \\mathbb{N}$. Equidistribution theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. The vectors of the form $[\\cos(wx_1), \\ldots, \\cos(wx_n)],\\, w \\in \\mathbb{N}$Â is dense in $[-1,1]^n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. Thus the network can produce any set of signs in $[-1,1]^n$ on $x_1,\\ldots,x_n$ by choosing $w \\in \\mathbb{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VC dimension of Neural networks\n",
    "Let $W$ be the total number of parameters of a network\n",
    "* (Cover 1968, Baum & Haussler 1989) LTU (Linear Threshold unit) \n",
    "$$\\VCdim \\leq 2 W \\log(eW)$$\n",
    "* (Sontag 1992) $\\phi(x) = \\frac{1}{\\pi} \\arctan(x)+ \\frac{1}{2} + \\frac{cos(x)}{\\alpha (1+x^2)}$\n",
    "$$\\VCdim = \\infty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* (Karpinski & Macintyre 1994) Sigmoid activation \n",
    "$$\\VCdim \\leq C W^4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* (Bartlett, Harvey, Liaw, Mehrabian 2017) Piecewise linear \n",
    "$$\\VCdim \\leq LW log(pU)$$\n",
    "Layers, Weights, pieces, Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The issue\n",
    "\n",
    "* In the overparametrized regime we cannot get any information from the VC generalization bound, since in that case $N$ is much smaller than the VC-dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  * Resnet110 is 1.7M parameters but performs really well on 50k images.\n",
    "  * Other well performing networks are roughly 1M parameters for 50k images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Open problems and observations\n",
    "* Numerical studies finds that sigmoid networks have bigger generalization gap as opposed to ReLU, when trained with SGD. Does this connect to the VC-dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Is there a regularizing effect of SGD which effectively reduces the VC-dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Other ways of measuring complexity? Rademacher Complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Are there reasonable ways of *a posteriori* determining the generalization error?\n",
    "    * For instance can we infer something like $R_{emp,D_0}(h) < \\epsilon$ implies that $\\mathbb{P}(R_{emp,D'}(h)) \\leq O(\\epsilon)$\n",
    "    * PAC Bayes bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rademacher complexity\n",
    "Given a sample $D=((x_1,y_1), \\dots, (x_m,y_m)) \\in \\mathbb{Z}^N$, $\\sigma$ is Rademacher distributed (random sign), define\n",
    "\n",
    "$$\n",
    "    {\\displaystyle \\operatorname {Rad} _{D}(\\H)={\\frac {1}{N}}\\mathbb {E} _{\\sigma }\\left[\\sup _{h\\in \\H}\\sum _{i=1}^{N}\\sigma _{i}L(h(x_i),y_{i})\\right]}\n",
    "$$\n",
    "\n",
    "where $L(h(x),y) = \\chi_{h(x) = y}$.\n",
    "\n",
    "$$\n",
    "    R(h) = \\E_{(x,y) \\sim \\mu}[h(x) \\neq y]\n",
    "$$\n",
    "Then for any hypothesis $h \\in \\H$ we have with probability at least $1-\\delta$\n",
    "$$\n",
    "    R(h) - R_{emp,D}(h) \\leq \\operatorname{Rad}_{D}(\\H) + 4 \\sqrt{\\frac{2\\log(4/\\delta)}{N}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Representativeness\n",
    "\n",
    "The representativeness can be defined as\n",
    "\n",
    "$$\n",
    "    Rep(D,\\H) = \\sup_{h \\in \\H} (R(h) - R_{emp,D}(h))\n",
    "$$\n",
    "\n",
    "If we split $D$ into two pieces $D_1,D_2$ we could consider estimating $Rep(D,\\H)$ as\n",
    "\n",
    "$$\n",
    "    \\sup_{h \\in \\H} (R_{emp,D_1}(h) - R_{emp,D_2}(h))\n",
    "$$\n",
    "\n",
    "this gives rise to the idea of Rademacher complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\H$ does not necessarily have to be the full space it can be a suitable subspace.\n",
    "* This implies that if we can find a good hypothesis $h$ and a neighbourhood of that with low Rademacher complexity then we get a generalization bound. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PAC Bayes bound (McAllester, 2002)\n",
    "For any data distribution $\\mu$ and any prior $P$ over $\\H$, we have the following bound $(S = \\{x_i | i = 1,\\ldots, N\\})$\n",
    "\n",
    "$$\n",
    "    \\mathbb{P}_S\\left (\\phi\\left (\\E_{h \\sim Q} R(h), \\E_{h \\sim Q} R_{emp,S}(h) \\right ) \\geq \\frac{D[Q || P] + \\log(2C\\beta) + \\log \\delta^{-1}}{\\beta-1} \\right ) < \\delta\n",
    "$$\n",
    "\n",
    "where $Q$ is an arbitrary posterior distribution over hypothesis. Provided that\n",
    "$$\n",
    "    \\mathbb{P}(R_{emp}(h) \\geq q) \\leq C e^{-\\beta\\phi(q,R(h))}, q \\geq R(h)\n",
    "$$\n",
    "$$\n",
    "    \\mathbb{P}(R_{emp}(h) \\leq q) \\leq C e^{-\\beta\\phi(q,R(h))}, q \\leq R(h)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "    {\\displaystyle D(Q\\parallel P)=\\int _{-\\infty }^{\\infty }q(x)\\log \\left({\\frac {q(x)}{q(x)}}\\right)\\,dx}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
