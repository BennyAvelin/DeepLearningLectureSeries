{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to deep learning\n",
    "Benny Avelin\n",
    "<p><a href=\"https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg#/media/File:Colored_neural_network.svg\">\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png\" width=500px alt=\"Colored neural network.svg\">\n",
    "        </a>\n",
    "        </center>\n",
    "        <br>\n",
    "        <font size=\"1\">By <a href=\"//commons.wikimedia.org/wiki/User_talk:Glosser.ca\" title=\"User talk:Glosser.ca\">Glosser.ca</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, Derivative of <a href=\"//commons.wikimedia.org/wiki/File:Artificial_neural_network.svg\" title=\"File:Artificial neural network.svg\">File:Artificial neural network.svg</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=24913461\">Link</a>\n",
    "    </p>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Definitions (Skip)\n",
    "$\\newcommand{\\mathbb{R}}{\\mathbb{R}}$\n",
    "$\\newcommand{\\mathbb{E}}{\\mathbb{E}}$\n",
    "$\\newcommand{\\H}{\\mathcal{H}}$\n",
    "$\\newcommand{\\VCdim}{\\text{VC-dim}(\\H)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview (session 4)\n",
    "\n",
    "* CNNs\n",
    "* ImageNet challenge and different winners\n",
    "* Autoencoders\n",
    "* Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# There are many different problems that NNs attempt to solve\n",
    "* Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Object location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Image restoration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Fingerprinting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNNs\n",
    "* When images or audio are involved Convolutional Neural Networks are almost exclusively used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Convolutional neural networks are an extension of the Neural Networks that we have defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We replace the vector dot-product with linear operators that are discrete convolutional operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNNs\n",
    "* Consider an $a \\times b$ convolutional kernel (matrix), $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Consider the input $X$ being $n \\times m$ matrix (think image)\n",
    "$$\n",
    "    (K \\ast X)_{ij} = \\sum_{k=1,l=1}^{a,b} K_{kl} X_{i-k,j-l}\n",
    "$$\n",
    "for $i = a+1,\\ldots, n$, $j=b+1, \\ldots, m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This is called `padding=valid` convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* There are dimension preserving convolutions (`padding = same`), this is done by extending $X$ as identically zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For one-dimension (usually time series) there is a `padding=causal`, such that it does not convolve with the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does this become a network?\n",
    "* A single artificial neuron can then be represented as\n",
    "$$\n",
    "    h(x) = \\sigma(w \\cdot x + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Replacing the dot-product $w \\cdot x$ with the convolution and instead of considering $x \\in \\mathbb{R}^d$ we consider $X \\in \\mathbb{R}^{n}\\times \\mathbb{R}^m$.\n",
    "* $\\sigma$ is applied componentwise as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNNs\n",
    "\n",
    "* A single CNN neuron can then be represented as\n",
    "$$\n",
    "    h(x) = \\sigma(K \\ast X + b)\n",
    "$$\n",
    "$b$Â is a number added to each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* But this is single input!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We define the input dimensions with how many `channels` the input has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Thus real input has shape $X \\in \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image data\n",
    "* Image data has this form, the first two coordinates \n",
    "signifies the pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The third coordinate is the `channel`, normally this is the colors, `RGB`, Red, Green, Blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Thus the full definition of a single CNN neuron with $d$ input-channels becomes\n",
    "$$\n",
    "    h(x) = \\sigma\\left (\\sum_{i=1}^d K_i \\ast X_i + b \\right )\n",
    "$$\n",
    "where $X_i$ is the $i$:th channel, and $K_i$ is the $i$:th channel kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What does the kernels look like?\n",
    "* AlexNet (Krizhevsky, Sutskever, Hinton, 2012)\n",
    "<img src=\"AlexNet.jpeg\" width=200%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AlexNet 2012\n",
    "#### Krizhevsky, Sutskever, Hinton\n",
    "<img src=\"AlexNetTopology.png\" width=900px>\n",
    "* Considered to be the most influential paper for using GPUs to train deep cnn networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ImageNet Large Scale Visual Recognition Challenge\n",
    "## (ILSVRC or ImageNet)\n",
    "* Over 14 Million hand annotated images, more than 20,000 categories.\n",
    "* Runs each year since 2010.\n",
    "<p><a href=\"https://commons.wikimedia.org/wiki/File:ImageNet_error_rate_history_(just_systems).svg#/media/File:ImageNet_error_rate_history_(just_systems).svg\"><center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ImageNet_error_rate_history_%28just_systems%29.svg/1200px-ImageNet_error_rate_history_%28just_systems%29.svg.png\" width=500px alt=\"ImageNet error rate history (just systems).svg\"></center></a><br>By <a href=\"//commons.wikimedia.org/w/index.php?title=User:Gkrusze&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"User:Gkrusze (page does not exist)\">Gkrusze</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=69750373\">Link</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2014 winner, GoogLeNet\n",
    "#### Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, Rabinovich\n",
    "![GoogLeNet](googlenet_diagram.png)\n",
    "* VGG16 was runner up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2015 winner, ResNet \n",
    "#### He, Zhang, Ren, Sun\n",
    "<center>\n",
    "    <img src=\"ResNetBlock.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ResNet\n",
    "<img src=\"ResNet.png\" widht=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ResNet\n",
    "<center>\n",
    "    <img src=\"ResNetBlock.png\" width=400px>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Call $y_{n-1}$ as the input\n",
    "* Call $\\mathcal{F}$ as the `weight+relu+weight`\n",
    "* Call $\\sigma$ the ReLU\n",
    "$$\n",
    "    y_n = \\sigma (y_{n-1} + \\mathcal{F}(y_{n-1}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ResNet\n",
    "* this is almost a discrete ODE, lets remove the outer ReLU\n",
    "$$\n",
    "    y_n-y_{n-1} = \\mathcal{F}(y_{n-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Euler discretization of the ODE\n",
    "$$\n",
    "    \\dot{y_t} = \\mathcal{F}(y_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NeuralODEs (2018)\n",
    "#### Chen, Rubanova, Bettencourt, Duvenaud\n",
    "$$\n",
    "    \\dot{y_t} = f(y_t,t,\\theta)\n",
    "$$\n",
    "* $f$ is the network\n",
    "* $y$ is the ODE solution\n",
    "* $\\theta$ are the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Problem is that if we discretize this and want to compute the gradient, the memory requirements scales with the number of time steps!\n",
    "* Their idea, use an adjoint ODE to compute the gradient of the loss, constant memory req."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autoencoders\n",
    "<p><a href=\"https://commons.wikimedia.org/wiki/File:Autoencoder_structure.png#/media/File:Autoencoder_structure.png\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png\" alt=\"Autoencoder structure.png\"></a><br>By <a href=\"//commons.wikimedia.org/w/index.php?title=User:Chervinskii&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"User:Chervinskii (page does not exist)\">Chervinskii</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=45555552\">Link</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autoencoders\n",
    "* Nonlinear \"compression\" or \"projection\" or \"sparse representation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Linear activations or Sigmoid+1 Hidden -> strong correlation to PCA, (Plaut, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> \"The weights of an autoencoder with a single hidden layer of size $p$ (where $p$ is less than the size of the input) span the same vector subspace as the one spanned by the first $p$ principal components, and the output of the autoencoder is an orthogonal projection onto this subspace.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autoencoders\n",
    "* Denoising Autoencoder (can be used for fingerprinting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sparse Autoencoder (Large hidden layer but sparse penalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Variational Autoencoder (Variational Bayesian approach, approximates the posterior, strong assumptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement learning\n",
    "* In its basic form its modeled as a *Markov decision process*\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"col-md-8\" markdown=\"1\">\n",
    "      <ul>\n",
    "          <li>Environment and agent state $S$</li>\n",
    "          <li>$P_a(s,s') = P(s_{t+1}=s' | s_t = s, a_t = a)>0$, transition probabilities</li>\n",
    "          <li>$R_a(s,s')$ immediate reward from being in state $s$ taking action $a$ and ending up in state $s'$.</li>\n",
    "          <li>$\\pi: S \\to A$ the policy</li>\n",
    "      </ul>\n",
    "  </div>\n",
    "  <div class=\"col-md-4\" markdown=\"1\">\n",
    "      <!-- ![Alt Text](../img/folder/blah.jpg) -->\n",
    "      <!-- <img height=\"600px\" class=\"center-block\" src=\"../img/folder/blah.jpg\"> -->\n",
    "      <p><a href=\"https://commons.wikimedia.org/wiki/File:Reinforcement_learning_diagram.svg#/media/File:Reinforcement_learning_diagram.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/1200px-Reinforcement_learning_diagram.svg.png\" alt=\"Reinforcement learning diagram.svg\" widht=200px class='center-block'></a><br><small>By <a href=\"//commons.wikimedia.org/w/index.php?title=User:Megajuice&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"User:Megajuice (page does not exist)\">Megajuice</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"http://creativecommons.org/publicdomain/zero/1.0/deed.en\" title=\"Creative Commons Zero, Public Domain Dedication\">CC0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=57895741\">Link</a></small></p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning (Watkins 1989)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In Q-Learning we do not know the transition probabilities so we need to experiment and learn it from experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "    V^\\pi(s) = \\mathbb{E} \\left [ \\sum_{t} \\gamma^t R_{\\pi(s_t)}(s_t,s_{t+1}) \\right ], \\quad s_0=s\n",
    "$$\n",
    "given $s_t$ we know that $s_{t+1}$ is a random variable with distribution $P_{\\pi(s_t)}(s_t,\\cdot)$, and $\\gamma < 1$ is a discount factor.\n",
    "* For a given policy the sequence of states $s_t$ starting in $s$ is a Markov process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning\n",
    "$$\n",
    "    V^\\ast(s) = \\max_{\\pi} V^{\\pi}(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We define for a given $s$ and $a$, \n",
    "$$\n",
    "    Q^\\ast(s,a) = \\max_\\pi Q^\\pi(s,a)\n",
    "$$\n",
    "where $Q^\\pi(s,a)$ is the expected reward starting at $s$ taking action $a$ and then following $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "    V^\\ast(s) = \\max_a Q^\\ast(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Bellman equation for MDPs states that\n",
    "$$\n",
    "    Q^\\ast(s,a) = \\mathbb{E}_{s' \\sim P_a(s,\\cdot)}[R_a(s,s') + \\gamma \\max_{a'} Q^\\ast(s',a')]\n",
    "$$\n",
    "where $s'$ is the state reached after taking action $a$ in state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Note the policy depends on the starting position $s$, but our assumption that the transition probability was positive now removes this dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning\n",
    "* Start with an initial guess for the function $Q: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$\n",
    "* Iterate as follows\n",
    "$$\\tiny{\\displaystyle Q^{new}(s_{t},a_{t})\\leftarrow (1-\\alpha )\\cdot \\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}+\\underbrace {\\alpha } _{\\text{learning rate}}\\cdot \\overbrace {{\\bigg (}\\underbrace {r_{t}} _{\\text{reward}}+\\underbrace {\\gamma } _{\\text{discount factor}}\\cdot \\underbrace {\\max _{a}Q(s_{t+1},a)} _{\\text{estimate of optimal future value}}{\\bigg )}} ^{\\text{learned value}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning\n",
    "* Q-Learning requires us to keep track of the matrix $Q$ which can be very large is the state and action space is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Neural Q-Learning\n",
    "* Approximate the $Q$ matrix with a deep neural network\n",
    "* Works, but the learning problem has too much correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Deep Q-Learning\n",
    "* DeepMind used a method called `experience replay` to decorrelate the training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notable achievements\n",
    "* Alpha Go, Learned from expert players then trained against itself. \n",
    "* Alpha Go Zero, trained entirely by playing against itelf. \n",
    "* AlphaZero, similar to above but better and general, can play several games.\n",
    "* DeepMind, Playing Atari with Deep Reinforcement Learning (2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next session\n",
    "\n",
    "* ?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
